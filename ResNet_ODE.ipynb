{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvRpIV6ISp2/tIjQ0mY+rs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmovahed/UDLbook_ResNet/blob/main/ResNet_ODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(1, 10)\n",
        "        self.fc2 = nn.Linear(10, 100)\n",
        "        self.fc3 = nn.Linear(100, 10)\n",
        "        self.fc4 = nn.Linear(10, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.fc1(x))\n",
        "        out = self.fc2(out) + x\n",
        "        out = self.fc3(out) + x\n",
        "        out = self.fc4(out) + x\n",
        "        return out\n",
        "\n",
        "\n",
        "def dy_dx(y, x):\n",
        "    return torch.autograd.grad(\n",
        "        y, x, grad_outputs=torch.ones_like(y), create_graph=True\n",
        "    )[0]\n",
        "\n",
        "x = torch.linspace(0, 1, 100, requires_grad=True)\n",
        "x = x.reshape(-1, 1)\n",
        "\n",
        "\n",
        "model = ResNet()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "losses = []\n",
        "for i in range(10000):\n",
        "\n",
        "    y = model.forward(x)\n",
        "    y_pred = dy_dx(y, x)\n",
        "    residual = y_pred - y\n",
        "    initial = y[0] - 1\n",
        "\n",
        "    loss = (residual**2).mean() + initial**2\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss.detach().numpy()[0])\n",
        "    if i % 100 == 0:\n",
        "        print(f'Loss at iteration {i}: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M4ZPEcWAuzB",
        "outputId": "f9e2679e-d3c4-40e6-c669-c6b34a1f908f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at iteration 0: 0.9747415781021118\n",
            "Loss at iteration 100: 0.030638905242085457\n",
            "Loss at iteration 200: 0.03931224346160889\n",
            "Loss at iteration 300: 0.05516015738248825\n",
            "Loss at iteration 400: 0.07328261435031891\n",
            "Loss at iteration 500: 0.09161370992660522\n",
            "Loss at iteration 600: 0.10152797400951385\n",
            "Loss at iteration 700: 0.11303877830505371\n",
            "Loss at iteration 800: 0.12441439926624298\n",
            "Loss at iteration 900: 0.13777989149093628\n",
            "Loss at iteration 1000: 0.1485760360956192\n",
            "Loss at iteration 1100: 0.15407200157642365\n",
            "Loss at iteration 1200: 0.15975697338581085\n",
            "Loss at iteration 1300: 0.15976974368095398\n",
            "Loss at iteration 1400: 0.1654735505580902\n",
            "Loss at iteration 1500: 0.16864116489887238\n",
            "Loss at iteration 1600: 0.17135018110275269\n",
            "Loss at iteration 1700: 0.17134569585323334\n",
            "Loss at iteration 1800: 0.1713458150625229\n",
            "Loss at iteration 1900: 0.1713515967130661\n",
            "Loss at iteration 2000: 0.17135700583457947\n",
            "Loss at iteration 2100: 0.1713482141494751\n",
            "Loss at iteration 2200: 0.17134739458560944\n",
            "Loss at iteration 2300: 0.1733236461877823\n",
            "Loss at iteration 2400: 0.17136460542678833\n",
            "Loss at iteration 2500: 0.17205575108528137\n",
            "Loss at iteration 2600: 0.17739570140838623\n",
            "Loss at iteration 2700: 0.17165397107601166\n",
            "Loss at iteration 2800: 0.17208534479141235\n",
            "Loss at iteration 2900: 0.171346053481102\n",
            "Loss at iteration 3000: 0.17140157520771027\n",
            "Loss at iteration 3100: 0.1679302603006363\n",
            "Loss at iteration 3200: 0.16776998341083527\n",
            "Loss at iteration 3300: 0.17136754095554352\n",
            "Loss at iteration 3400: 0.17135104537010193\n",
            "Loss at iteration 3500: 0.1713569313287735\n",
            "Loss at iteration 3600: 0.1716608703136444\n",
            "Loss at iteration 3700: 0.17382964491844177\n",
            "Loss at iteration 3800: 0.17136721312999725\n",
            "Loss at iteration 3900: 0.1773451417684555\n",
            "Loss at iteration 4000: 0.1773546040058136\n",
            "Loss at iteration 4100: 0.17139218747615814\n",
            "Loss at iteration 4200: 0.17203660309314728\n",
            "Loss at iteration 4300: 0.17145897448062897\n",
            "Loss at iteration 4400: 0.17135551571846008\n",
            "Loss at iteration 4500: 0.17737899720668793\n",
            "Loss at iteration 4600: 0.171458438038826\n",
            "Loss at iteration 4700: 0.1713598668575287\n",
            "Loss at iteration 4800: 0.1676749438047409\n",
            "Loss at iteration 4900: 0.17169253528118134\n",
            "Loss at iteration 5000: 0.17736344039440155\n",
            "Loss at iteration 5100: 0.1716318130493164\n",
            "Loss at iteration 5200: 0.17174012959003448\n",
            "Loss at iteration 5300: 0.17189843952655792\n",
            "Loss at iteration 5400: 0.17174728214740753\n",
            "Loss at iteration 5500: 0.1719779074192047\n",
            "Loss at iteration 5600: 0.17145539820194244\n",
            "Loss at iteration 5700: 0.16772329807281494\n",
            "Loss at iteration 5800: 0.1682892143726349\n",
            "Loss at iteration 5900: 0.17738352715969086\n",
            "Loss at iteration 6000: 0.1655052900314331\n",
            "Loss at iteration 6100: 0.17147330939769745\n",
            "Loss at iteration 6200: 0.15424329042434692\n",
            "Loss at iteration 6300: 0.17137280106544495\n",
            "Loss at iteration 6400: 0.17969295382499695\n",
            "Loss at iteration 6500: 0.16753247380256653\n",
            "Loss at iteration 6600: 0.17374016344547272\n",
            "Loss at iteration 6700: 0.1618761569261551\n",
            "Loss at iteration 6800: 0.17191950976848602\n",
            "Loss at iteration 6900: 469.0071105957031\n",
            "Loss at iteration 7000: 0.07302968949079514\n",
            "Loss at iteration 7100: 0.059751879423856735\n",
            "Loss at iteration 7200: 0.05973294377326965\n",
            "Loss at iteration 7300: 0.05973150208592415\n",
            "Loss at iteration 7400: 0.05973110720515251\n",
            "Loss at iteration 7500: 0.059730809181928635\n",
            "Loss at iteration 7600: 0.05973010137677193\n",
            "Loss at iteration 7700: 0.059729188680648804\n",
            "Loss at iteration 7800: 0.05972830206155777\n",
            "Loss at iteration 7900: 0.05972772836685181\n",
            "Loss at iteration 8000: 0.059726882725954056\n",
            "Loss at iteration 8100: 0.059726253151893616\n",
            "Loss at iteration 8200: 0.05972515046596527\n",
            "Loss at iteration 8300: 0.05972416698932648\n",
            "Loss at iteration 8400: 0.059722959995269775\n",
            "Loss at iteration 8500: 0.059722136706113815\n",
            "Loss at iteration 8600: 0.05972112715244293\n",
            "Loss at iteration 8700: 0.05972020700573921\n",
            "Loss at iteration 8800: 0.059718966484069824\n",
            "Loss at iteration 8900: 0.05971786007285118\n",
            "Loss at iteration 9000: 0.05971669778227806\n",
            "Loss at iteration 9100: 0.05971500277519226\n",
            "Loss at iteration 9200: 0.059713419526815414\n",
            "Loss at iteration 9300: 0.059712089598178864\n",
            "Loss at iteration 9400: 0.05971043184399605\n",
            "Loss at iteration 9500: 0.05970869958400726\n",
            "Loss at iteration 9600: 0.05970685929059982\n",
            "Loss at iteration 9700: 0.0597052238881588\n",
            "Loss at iteration 9800: 0.059702906757593155\n",
            "Loss at iteration 9900: 0.059701189398765564\n"
          ]
        }
      ]
    }
  ]
}