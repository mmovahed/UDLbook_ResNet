{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2rqE+wQ7i2JpDfQUWJLqr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmovahed/UDLbook_ResNet/blob/main/Shattered_Gradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VDc_5I8mj3QA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# K is width, D is number of hidden units in each layer\n",
        "def init_params(K, D):\n",
        "  # Set seed so we always get the same random numbers\n",
        "  np.random.seed(1)\n",
        "\n",
        "  # Input layer\n",
        "  D_i = 1\n",
        "  # Output layer\n",
        "  D_o = 1\n",
        "\n",
        "  # Glorot initialization\n",
        "  sigma_sq_omega = 1.0/D\n",
        "\n",
        "  # Make empty lists\n",
        "  all_weights = [None] * (K+1)\n",
        "  all_biases = [None] * (K+1)\n",
        "\n",
        "  # Create parameters for input and output layers\n",
        "  all_weights[0] = np.random.normal(size=(D, D_i))*np.sqrt(sigma_sq_omega)\n",
        "  all_weights[-1] = np.random.normal(size=(D_o, D)) * np.sqrt(sigma_sq_omega)\n",
        "  all_biases[0] = np.random.normal(size=(D,1))* np.sqrt(sigma_sq_omega)\n",
        "  all_biases[-1]= np.random.normal(size=(D_o,1))* np.sqrt(sigma_sq_omega)\n",
        "\n",
        "  # Create intermediate layers\n",
        "  for layer in range(1,K):\n",
        "    all_weights[layer] = np.random.normal(size=(D,D))*np.sqrt(sigma_sq_omega)\n",
        "    all_biases[layer] = np.random.normal(size=(D,1))* np.sqrt(sigma_sq_omega)\n",
        "\n",
        "  return all_weights, all_biases\n",
        "\n",
        "  # Define the Rectified Linear Unit (ReLU) function\n",
        "def ReLU(preactivation):\n",
        "  activation = preactivation.clip(0.0)\n",
        "  return activation\n",
        "\n",
        "def forward_pass(net_input, all_weights, all_biases):\n",
        "\n",
        "  # Retrieve number of layers\n",
        "  K = len(all_weights) -1\n",
        "\n",
        "  # We'll store the pre-activations at each layer in a list \"all_f\"\n",
        "  # and the activations in a second list[all_h].\n",
        "  all_f = [None] * (K+1)\n",
        "  all_h = [None] * (K+1)\n",
        "\n",
        "  #For convenience, we'll set\n",
        "  # all_h[0] to be the input, and all_f[K] will be the output\n",
        "  all_h[0] = net_input\n",
        "\n",
        "  # Run through the layers, calculating all_f[0...K-1] and all_h[1...K]\n",
        "  for layer in range(K):\n",
        "      # Update preactivations and activations at this layer according to eqn 7.5\n",
        "      all_f[layer] = all_biases[layer] + np.matmul(all_weights[layer], all_h[layer])\n",
        "      all_h[layer+1] = ReLU(all_f[layer])\n",
        "\n",
        "  # Compute the output from the last hidden layer\n",
        "  all_f[K] = all_biases[K] + np.matmul(all_weights[K], all_h[K])\n",
        "\n",
        "  # Retrieve the output\n",
        "  net_output = all_f[K]\n",
        "\n",
        "  return net_output, all_f, all_h\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll need the indicator function\n",
        "def indicator_function(x):\n",
        "  x_in = np.array(x)\n",
        "  x_in[x_in>=0] = 1\n",
        "  x_in[x_in<0] = 0\n",
        "  return x_in\n",
        "\n",
        "# Main backward pass routine\n",
        "def calc_input_output_gradient(x_in, all_weights, all_biases):\n",
        "\n",
        "  # Run the forward pass\n",
        "  y, all_f, all_h = forward_pass(x_in, all_weights, all_biases)\n",
        "\n",
        "  # We'll store the derivatives dl_dweights and dl_dbiases in lists as well\n",
        "  all_dl_dweights = [None] * (K+1)\n",
        "  all_dl_dbiases = [None] * (K+1)\n",
        "  # And we'll store the derivatives of the loss with respect to the activation and preactivations in lists\n",
        "  all_dl_df = [None] * (K+1)\n",
        "  all_dl_dh = [None] * (K+1)\n",
        "  # Again for convenience we'll stick with the convention that all_h[0] is the net input and all_f[k] in the net output\n",
        "\n",
        "  # Compute derivatives of net output with respect to loss\n",
        "  all_dl_df[K] = np.ones_like(all_f[K])\n",
        "\n",
        "  # Now work backwards through the network\n",
        "  for layer in range(K,-1,-1):\n",
        "    all_dl_dbiases[layer] = np.array(all_dl_df[layer])\n",
        "    all_dl_dweights[layer] = np.matmul(all_dl_df[layer], all_h[layer].transpose())\n",
        "\n",
        "    all_dl_dh[layer] = np.matmul(all_weights[layer].transpose(), all_dl_df[layer])\n",
        "\n",
        "    if layer > 0:\n",
        "      all_dl_df[layer-1] = indicator_function(all_f[layer-1]) * all_dl_dh[layer]\n",
        "\n",
        "\n",
        "  return all_dl_dh[0],y"
      ],
      "metadata": {
        "id": "xc2BKKs4kjBY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D = 200; K = 3\n",
        "# Initialize parameters\n",
        "all_weights, all_biases = init_params(K,D)\n",
        "\n",
        "x = np.ones((1,1))\n",
        "dydx,y = calc_input_output_gradient(x, all_weights, all_biases)\n",
        "\n",
        "# Offset for finite gradients\n",
        "delta = 0.00000001\n",
        "x1 = x\n",
        "y1,*_ = forward_pass(x1, all_weights, all_biases)\n",
        "x2 = x+delta\n",
        "y2,*_ = forward_pass(x2, all_weights, all_biases)\n",
        "# Finite difference calculation\n",
        "dydx_fd = (y2-y1)/delta\n",
        "\n",
        "print(\"Gradient calculation=%f, Finite difference gradient=%f\"%(dydx.squeeze(),dydx_fd.squeeze()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PefwcdwGklW_",
        "outputId": "96536337-2b4c-4e1c-a5ad-06a344334f62"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient calculation=0.013250, Finite difference gradient=0.013250\n"
          ]
        }
      ]
    }
  ]
}